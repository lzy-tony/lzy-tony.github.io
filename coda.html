<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CODA: Repurposing Continuous VAEs for Discrete Tokenization">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CODA: Repurposing Continuous VAEs for Discrete Tokenization
  </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/img/COLE.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

<style>
  .publication-title {
    font-family: "textsc "
  }
  video {
    outline: none;
    border: none;
    box-shadow: none;
  }
  .image_wrapper {
    position: relative;
    width: 226px;
    height: 226px;
  }

  .image_container {
    border-radius: 10px; /*设置图片的圆角，可以根据你的需求调整这个值*/
    /*margin: 10px; !* 在图片的外部元素上设置间距 *!*/
    overflow: hidden; /* 为了保持图片的圆角效果 */
    position: relative;
    width: 100%;

    padding-bottom: 100%;
  }

  .image_container img {

    position: absolute;
    width: 100%;
    height: 100%;
    object-fit: cover; /* 这会使图片保持原始比例，但仍然填充其容器 */
    object-position: center; /* 新增加的代码，让图片居中 */
  }

  .caption {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(0, 0, 0, 0.5);
    color: white;
    display: flex;
    align-items: center; /* 垂直居中 */
    justify-content: center; /* 水平居中 */
    font-family: 'Comic Sans MS', 'Verdana', 'Tahoma', sans-serif;
    text-align: center; /* 让文本在其容器中水平居中 */
    opacity: 0;
    transition: opacity 0.5s ease;
  }

  .image_container:hover .caption {
    opacity: 1;
  }



  .column.is-2.has-text-centered img.interpolation-image {

    object-fit: cover;
    border-radius: 10px;
    transition: opacity 0.5s ease;
    object-position: center; /* 新增加的代码，让图片居中 */
  }

  .column.is-2.has-text-centered:hover img.interpolation-image {
    opacity: 0.5;
  }

  .column.is-2.has-text-centered .caption {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(0, 0, 0, 0.5);
    color: white;
    display: flex;
    align-items: center; /* 垂直居中 */
    justify-content: center; /* 水平居中 */
    font-family: 'Comic Sans MS', 'Verdana', 'Tahoma', sans-serif;
    text-align: center; /* 让文本在其容器中水平居中 */
    opacity: 0;
    transition: opacity 0.5s ease;
  }

  .column.is-2.has-text-centered:hover .caption {
    opacity: 1;
  }

  .columns.is-centered img {
    margin-bottom: 20px; /* 或者你想要的任何值 */
  }


  .section { margin: 3px; padding: 5px; }
</style>

<!--<h1 class="title is-1 publication-title">COLE :A Hierarchical Generation Framework for Graphic Design</h1>-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 ">CODA: Repurposing Continuous VAEs for Discrete Tokenization</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://lzy-tony.github.io/">Zeyu Liu</a><sup>1*</sup>&nbsp&nbsp&nbsp</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Yibz_asAAAAJ&hl=en">Zanlin Ni</a><sup>1*</sup>&nbsp&nbsp&nbsp</span>
            <span class="author-block">
              <a>Yeguo Hua</a><sup>1</sup>&nbsp&nbsp&nbsp
            </span>
            <span class="author-block">
              <a>Xin Deng</a><sup>2</sup>&nbsp&nbsp&nbsp
            </span>
            <span class="author-block">
              <a>Xiao Ma</a><sup>3</sup>&nbsp&nbsp&nbsp
            </span>
            <span class="author-block">
              <a>Cheng Zhong</a><sup>3</sup>&nbsp&nbsp&nbsp
            </span>
            <span class="author-block">
              <a href="https://www.gaohuang.net/">Gao Huang</a><sup>1†</sup>&nbsp&nbsp&nbsp
            </span>
            <br>
            <span class="is-size-6 publication-authors">
              <sup>*</sup><span>Equal Contribution&nbsp&nbsp&nbsp</span>
            </span>
            <span class="is-size-6 publication-authors">
              <sup>†</sup><span>Corresponding Author&nbsp&nbsp&nbsp</span>
            </span>


          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>2</sup>Renmin University&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>3</sup>Lenovo Research, AI Lab&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="http://arxiv.org/abs/2503.17760"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
	      </span>

  	      <span class="link-block">
                <a href="https://github.com/LeapLabTHU/CODA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

	    </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Idea</h2>
        <img src="./static/assets/coda/main_idea.svg" alt="main idea" style="width: 80%;">
        <p align="left">
          <b>(a) Conventional discrete VQ tokenizers</b> learn to <b>compress</b> and <b>discretize</b> inherently continuous visual signals into codes simultaneously. This lead to multiple challenges in training and the corresponding unsatisfactory latent space poses a bottleneck that limit the performance of discrete token-based generation models. 
        </p>
        <p align="left">
          <b>(b) Our proposed CODA tokenizer</b> leverages continuous VAEs for compression, directly discretizing the latent space.
        </p>
        <p align="left">
          <b>(c) Quantitative comparisons</b> between VQGAN and our proposed CODA tokenizer.
        </p>
      </div>
    </div>
  
    <br>
    <br>
  
  

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both <b>compressing</b> visual signals into a compact representation and <b>discretizing</b> them into a fixed set of codes.
          </p>
          <p>
            Traditional discrete tokenizers typically learn the two tasks jointly, often leading to unstable training, low codebook utilization, and limited reconstruction quality. In this paper, we introduce <b>CODA</b> (<b>CO</b>ntinuous-to-<b>D</b>iscrete <b>A</b>daptation), a framework that decouples compression and discretization.
          </p>
          <p>
            Instead of training discrete tokenizers from scratch, <b>CODA</b> adapts off-the-shelf continuous VAEs --- already optimized for perceptual compression --- into discrete tokenizers via a carefully designed discretization process.
            By primarily focusing on discretization, <b>CODA</b> ensures stable and efficient training while retaining the strong visual fidelity of continuous VAEs.
          </p>
          <p>
            Empirically, with 6x less training budget than standard VQGAN, our approach achieves a remarkable codebook utilization of <b>100%</b> and notable reconstruction FID (rFID) of <b>0.43</b> and <b>1.34</b> for 8x and 16x compression on ImageNet 256 x 256 benchmark.
          </p>
        </div>
      </div>
    </div>
  </div>
  
  <br>
  <br>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Approach</h2>
      <h3 class="title is-5">Enhanced Representational Capacity</h3>
      <img src="./static/assets/coda/capacity.svg" alt="capacity" style="width: 60%;">
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <p align="left">
          <b>Left</b>: <b>Visualization of latent space approximation and lack of representational capacity</b>: (a) the original latent space of the continuous VAE, (b) latent space approximated by vector quantization and (c) latent space approximated by residual quantization. This discrepancy indicate a <b>substantial information loss during the VQ approximation process.</b>
        </p>
        <br>
        <p align="left">
          <b>Right</b>: <b>Effect of residual quantization levels and enhanced representational capacity on tokenizer performance.</b> With more levels of residual quantization, quantization error is consistently minimized, and the reconstruction performance (measured by rFID) steadily improves.
        </p>
      </div>
    </div>
  </div>

  <br>
  <br>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h3 class="title is-5">Sparse Unambiguous Assignment</h3>
      <img src="./static/assets/coda/sparsity.svg" alt="sparsity" style="width: 60%;">
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <p align="left">
          <b>Left</b>: <b>Visualization of top assignment confidence scores</b>: for 16 randomly selected continuous VAE features. For vector quantization, we visualize the distance of codes to the continuous feature, with lower distance representing higher confidence. This reveals a clear pattern of <b>ambiguity</b> in code assignment.
        </p>
        <br>
        <p align="left">
          <b>Right</b>: <b>Visualization of training dynamics.</b> With the proposed sparse attention-based assignment, codes are <b>pushed</b> to fully occupy the latent space, whereas vector quantization shows limited coverage of latent space. This enforces more sufficient latent space coverage and unambiguous codebook assignment, along with enhanced training dynamics.
        </p>
      </div>
    </div>
  </div>

  <br>
  <br>


  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Pipeline</h2>
    </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <img src="./static/assets/coda/pipeline.svg" alt="pipeline" style="width: 60%;">
      <p>
        <b>Illustration of our CODA tokenizer</b>
      </p>
    </div>
  </div>
  

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <p style="text-align: left;">
          <b>(a)</b> a residual quantization process of L levels iteratively refines the approximation of a continuous VAE vector f through a composite of multiple quantization layers, thus progressively minimizing the quantization error. Meanwhile, as the continuous VAE vector is approximated by a <b>combination</b> of L discrete codes, the representational capacity is significantly enlarged.
        </p>
        <p style="text-align: left;">
          <b>(b)</b> the attention-based quantization process frames discretization as a <b>retrieval</b> task. Continuous features and codebook embeddings are projected and normalized onto a normed hypersphere, where the softmax attention matrix is computed to determine the confidence of code selection. As codes compete within the softmax attention framework, this approach ensures a <b>sparse</b> and <b>unambiguous</b> assignment.
        </p>
      </div>
    </div>
  </div>


  <br>
  <br>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Main Results</h2>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img src="./static/assets/coda/reconstruction.svg" alt="pipeline" style="width: 100%;">
        <p>
          <b>Reconstruction results on ImageNet</b>
        </p>
      </div>
    </div>
  </div>

  <br>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img src="./static/assets/coda/generation.svg" alt="pipeline" style="width: 60%;">
        <p>
          <b>Generation results on ImageNet</b>
        </p>
      </div>
    </div>
  </div>


  <br>
  <br>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Visualizations</h2>
    </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <img src="./static/assets/coda/visualization.svg" alt="pipeline" style="width: 60%;">
    </div>
  </div>

</section>
<br>
<br>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      Coming Soon.
    </code></pre>
  </div>
</section>


<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    <br>
    Website adapted from the following <a href="https://nerfies.github.io/">template</a>.
  </div>
</section>

</body>
</html>
